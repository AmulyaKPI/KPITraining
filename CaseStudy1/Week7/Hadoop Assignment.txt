1.load the given textfile in HDFS. 



[cloudera@quickstart ~]$ hdfs dfs -ls
Found 24 items
drwx------   - cloudera cloudera          0 2022-01-20 23:40 .staging
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 avro_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 csv_dir
-rw-r--r--   1 cloudera cloudera         44 2022-01-20 23:43 day1.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-12 09:57 emp
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 orc_dir
drwxr-xr-x   - cloudera cloudera          0 2022-01-20 23:40 output
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 part_dir2
-rw-r--r--   1 cloudera cloudera         84 2022-01-13 02:29 practice.csv
-rw-r--r--   1 cloudera cloudera         32 2022-01-11 03:13 sample.csv
-rw-r--r--   1 cloudera cloudera         31 2022-01-10 01:00 sample.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-16 06:14 student
-rw-r--r--   1 cloudera cloudera       1173 2022-01-20 22:44 words.txt
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 zeyo_dir
[cloudera@quickstart ~]$ cat words.txt
cat: words.txt: No such file or directory
[cloudera@quickstart ~]$ dir hadoop
day1.txt  HDFS\ practice  mobiles.csv  practice.csv  sample1.txt  words.txt
[cloudera@quickstart ~]$ cat words.txt
cat: words.txt: No such file or directory
[cloudera@quickstart ~]$ cd hadoop
[cloudera@quickstart hadoop]$ cat words.txt
It's a truly pleasant experience to read this book, actually I should confess that I laughed A LOT in the reading. The book is hilarious.

Besides the fun part, I was inspired by this book too. This book went through the early history of Personal Computer industry, gave the vivid silhouettes of the people, the companies and Silicon Valley in this industry. Mr.Cringely examined why today's Information Technology industry is what it is now, and how it became like this.

The book provided the facts and opinion about how the high tech companies succeeded, and how many more failed. Why Bill Gates is the richest person in the world, and how Steve Jobs and Steve Wozniak created the most beloved high tech company in the world.

It used to say that reading history can make people understand the rise and fall of things. We can learn the lessons from it, and get new ideas or patterns from the past success. Today Personal Computer is declining, and the focus is shifting to Smart Phone and Tablet. Although product is changing, the similar struggles, fights, winning and loss are still happening lively everyday in this industry, just like what it did in the old days.


[cloudera@quickstart hadoop]$ hdfs dfs -ls user/cloudera
ls: `user/cloudera': No such file or directory
[cloudera@quickstart hadoop]$ cd
[cloudera@quickstart ~]$ hdfs dfs -ls user/cloudera
ls: `user/cloudera': No such file or directory
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera
Found 24 items
drwx------   - cloudera cloudera          0 2022-01-20 23:40 /user/cloudera/.staging
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 /user/cloudera/avro_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 /user/cloudera/csv_dir
-rw-r--r--   1 cloudera cloudera         44 2022-01-20 23:43 /user/cloudera/day1.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-12 09:57 /user/cloudera/emp
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 /user/cloudera/import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 /user/cloudera/json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 /user/cloudera/json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 /user/cloudera/json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 /user/cloudera/orc_dir
drwxr-xr-x   - cloudera cloudera          0 2022-01-20 23:40 /user/cloudera/output
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 /user/cloudera/parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 /user/cloudera/parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 /user/cloudera/part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 /user/cloudera/part_dir2
-rw-r--r--   1 cloudera cloudera         84 2022-01-13 02:29 /user/cloudera/practice.csv
-rw-r--r--   1 cloudera cloudera         32 2022-01-11 03:13 /user/cloudera/sample.csv
-rw-r--r--   1 cloudera cloudera         31 2022-01-10 01:00 /user/cloudera/sample.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-16 06:14 /user/cloudera/student
-rw-r--r--   1 cloudera cloudera       1173 2022-01-20 22:44 /user/cloudera/words.txt
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 /user/cloudera/zeyo_dir
[cloudera@quickstart ~]$ hdfs dfs -cat words.txt /user/cloudera
It's a truly pleasant experience to read this book, actually I should confess that I laughed A LOT in the reading. The book is hilarious.

Besides the fun part, I was inspired by this book too. This book went through the early history of Personal Computer industry, gave the vivid silhouettes of the people, the companies and Silicon Valley in this industry. Mr.Cringely examined why today's Information Technology industry is what it is now, and how it became like this.

The book provided the facts and opinion about how the high tech companies succeeded, and how many more failed. Why Bill Gates is the richest person in the world, and how Steve Jobs and Steve Wozniak created the most beloved high tech company in the world.

It used to say that reading history can make people understand the rise and fall of things. We can learn the lessons from it, and get new ideas or patterns from the past success. Today Personal Computer is declining, and the focus is shifting to Smart Phone and Tablet. Although product is changing, the similar struggles, fights, winning and loss are still happening lively everyday in this industry, just like what it did in the old days.



2.Perform WordCount on the text file using mapreduce.




[cloudera@quickstart ~]$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-map-reduce-examples.jar
Not a valid JAR: /usr/lib/hadoop-mapreduce/hadoop-map-reduce-examples.jar
[cloudera@quickstart ~]$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/cloudera/words.txt /user/cloudera/output
22/01/20 23:36:47 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032
22/01/20 23:36:57 INFO input.FileInputFormat: Total input paths to process : 1
22/01/20 23:36:59 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/20 23:36:59 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/20 23:36:59 INFO mapreduce.JobSubmitter: number of splits:1
22/01/20 23:37:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1642739418076_0002
22/01/20 23:37:16 INFO impl.YarnClientImpl: Submitted application application_1642739418076_0002
22/01/20 23:37:18 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1642739418076_0002/
22/01/20 23:37:18 INFO mapreduce.Job: Running job: job_1642739418076_0002
22/01/20 23:39:06 INFO mapreduce.Job: Job job_1642739418076_0002 running in uber mode : false
22/01/20 23:39:06 INFO mapreduce.Job:  map 0% reduce 0%
22/01/20 23:40:03 INFO mapreduce.Job:  map 100% reduce 0%
22/01/20 23:40:18 INFO mapreduce.Job:  map 100% reduce 100%
22/01/20 23:40:19 INFO mapreduce.Job: Job job_1642739418076_0002 completed successfully
22/01/20 23:40:20 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=1261
		FILE: Number of bytes written=297411
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1293
		HDFS: Number of bytes written=1143
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=24435200
		Total time spent by all reduces in occupied slots (ms)=5496832
		Total time spent by all map tasks (ms)=47725
		Total time spent by all reduce tasks (ms)=10736
		Total vcore-milliseconds taken by all map tasks=47725
		Total vcore-milliseconds taken by all reduce tasks=10736
		Total megabyte-milliseconds taken by all map tasks=24435200
		Total megabyte-milliseconds taken by all reduce tasks=5496832
	Map-Reduce Framework
		Map input records=9
		Map output records=203
		Map output bytes=1980
		Map output materialized bytes=1257
		Input split bytes=120
		Combine input records=203
		Combine output records=134
		Reduce input groups=134
		Reduce shuffle bytes=1257
		Reduce input records=134
		Reduce output records=134
		Spilled Records=268
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=1324
		CPU time spent (ms)=2900
		Physical memory (bytes) snapshot=203337728
		Virtual memory (bytes) snapshot=3890036736
		Total committed heap usage (bytes)=101449728
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1173
	File Output Format Counters 
		Bytes Written=1143
[cloudera@quickstart ~]$ hadoop dfs -ls /user/cloudera/output
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2022-01-20 23:40 /user/cloudera/output/_SUCCESS
-rw-r--r--   1 cloudera cloudera       1143 2022-01-20 23:40 /user/cloudera/output/part-r-00000
[cloudera@quickstart ~]$ hadoop dfs -cat /user/cloudera/output/part-r-00000
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

A	1
Although	1
Besides	1
Bill	1
Computer	2
Gates	1
I	3
Information	1
It	1
It's	1
Jobs	1
LOT	1
Mr.Cringely	1
Personal	2
Phone	1
Silicon	1
Smart	1
Steve	2
Tablet.	1
Technology	1
The	2
This	1
Today	1
Valley	1
We	1
Why	1
Wozniak	1
a	1
about	1
actually	1
and	11
are	1
became	1
beloved	1
book	4
book,	1
by	1
can	2
changing,	1
companies	2
company	1
confess	1
created	1
days.	1
declining,	1
did	1
early	1
everyday	1
examined	1
experience	1
facts	1
failed.	1
fall	1
fights,	1
focus	1
from	2
fun	1
gave	1
get	1
happening	1
high	2
hilarious.	1
history	2
how	4
ideas	1
in	6
industry	1
industry,	2
industry.	1
inspired	1
is	7
it	3
it,	1
just	1
laughed	1
learn	1
lessons	1
like	2
lively	1
loss	1
make	1
many	1
more	1
most	1
new	1
now,	1
of	3
old	1
opinion	1
or	1
part,	1
past	1
patterns	1
people	1
people,	1
person	1
pleasant	1
product	1
provided	1
read	1
reading	1
reading.	1
richest	1
rise	1
say	1
shifting	1
should	1
silhouettes	1
similar	1
still	1
struggles,	1
succeeded,	1
success.	1
tech	2
that	2
the	18
things.	1
this	4
this.	1
through	1
to	3
today's	1
too.	1
truly	1
understand	1
used	1
vivid	1
was	1
went	1
what	2
why	1
winning	1
world,	1
world.	1
[cloudera@quickstart ~]$ 


3.Create a HBase table ‘Census’ using java with Column Family as ‘Personal’, ‘Professional’. 


cloudera@quickstart ~]$ hbase shell
OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
22/01/21 01:29:53 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Type "exit<RETURN>" to leave the HBase Shell
Version 1.2.0-cdh5.13.0, rUnknown, Wed Oct  4 11:16:18 PDT 2017
hbase(main):002:0> create 'census', 'personal', 'professional'
0 row(s) in 3.3680 seconds

=> Hbase::Table - census
hbase(main):004:0> describe 'census'
Table census is ENABLED                                                                                                                                                                                             
census                                                                                                                                                                                                              
COLUMN FAMILIES DESCRIPTION                                                                                                                                                                                         
{NAME => 'personal', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKC
ACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}                                                                                                                                                     
{NAME => 'professional', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BL
OCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}                                                                                                                                                 
2 row(s) in 0.5510 seconds



4.Put 2 rows in the Census table each having columns name and gender in personal and occupation in professional  and display data using HBase shell.


hbase(main):014:0> put 'census', '1', 'personal:name,gender', 'amulya,female'
0 row(s) in 0.1700 seconds

hbase(main):015:0> put 'census', '1', 'professional:occupation', 'trainee'
0 row(s) in 0.0150 seconds

hbase(main):016:0> put 'census', '2', 'personal:name,gender', 'jack,male'
0 row(s) in 0.0780 seconds

hbase(main):017:0> put 'census', '2', 'professional:occupation', 'senior trainee'
0 row(s) in 0.0380 seconds
hbase(main):001:0> scan 'census'
ROW                                                    COLUMN+CELL                                                                                                                                                  
 1                                                     column=personal:name,gender, timestamp=1642947274055, value=amulya,female                                                                                    
 1                                                     column=professional:occupation, timestamp=1642947287127, value=Trainee                                                                                       
 2                                                     column=personal:name,gender, timestamp=1642947301963, value=jack,male                                                                                        
 2                                                     column=professional:occupation, timestamp=1642947367799, value=senior trainee                                                                                
2 row(s) in 0.7460 seconds



5.Load the groceries data file using hbase,hive,sqoop in Grunt shell with a schema and describe and display the data.


[cloudera@quickstart ~]$ hdfs dfs -ls
Found 24 items
drwx------   - cloudera cloudera          0 2022-01-20 23:40 .staging
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 avro_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 csv_dir
-rw-r--r--   1 cloudera cloudera         44 2022-01-20 23:43 day1.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-12 09:57 emp
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 orc_dir
drwxr-xr-x   - cloudera cloudera          0 2022-01-20 23:40 output
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 part_dir2
-rw-r--r--   1 cloudera cloudera         84 2022-01-13 02:29 practice.csv
-rw-r--r--   1 cloudera cloudera         32 2022-01-11 03:13 sample.csv
-rw-r--r--   1 cloudera cloudera         31 2022-01-10 01:00 sample.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-16 06:14 student
-rw-r--r--   1 cloudera cloudera       1173 2022-01-20 22:44 words.txt
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 zeyo_dir
[cloudera@quickstart ~]$ cd hadoop
[cloudera@quickstart hadoop]$ vi groceries.csv
[cloudera@quickstart hadoop]$ cat groceries.csv
1,Seattle,Bananas,1/1/2017,7
2,Ken,Apples,1/2/2017,20
3,Bellevue,Flowers,1/2/2017,10
4,Redmond,Meat,1/3/2017,40
5,Seattle,Potatoes,1/4/2017,9
6,Bellevue,Bread,1/4/2017,5
7,Redmond,Bread,1/5/2017,5
8,Issaquah,Onion,1/5/2017,4
9,Redmond,Cheese,1/5/2017,15
10,Issaquah,Onion,1/6/2017,4
11,Renton,Bread,1/5/2017,5
12,Issaquah,Onion,1/7/2017,4
13,Sammamish,Bread,1/7/2017,5
14,Issaquah,Tomato,1/7/2017,6
[cloudera@quickstart hadoop]$ ls
day1.txt  groceries.csv  HDFS practice  mobiles.csv  practice.csv  sample1.txt  words.txt
[cloudera@quickstart hadoop]$ cd
[cloudera@quickstart ~]$ hdfs dfs -put groceries.csv /user/cloudera
hdfs[cloudera@quickstart ~]$ 
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera
Found 25 items
drwx------   - cloudera cloudera          0 2022-01-20 23:40 /user/cloudera/.staging
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 /user/cloudera/avro_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 /user/cloudera/csv_dir
-rw-r--r--   1 cloudera cloudera         44 2022-01-20 23:43 /user/cloudera/day1.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-12 09:57 /user/cloudera/emp
-rw-r--r--   1 cloudera cloudera        428 2022-01-23 09:22 /user/cloudera/groceries.csv
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 /user/cloudera/import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 /user/cloudera/json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 /user/cloudera/json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 /user/cloudera/json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 /user/cloudera/orc_dir
drwxr-xr-x   - cloudera cloudera          0 2022-01-20 23:40 /user/cloudera/output
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 /user/cloudera/parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 /user/cloudera/parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 /user/cloudera/part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 /user/cloudera/part_dir2
-rw-r--r--   1 cloudera cloudera         84 2022-01-13 02:29 /user/cloudera/practice.csv
-rw-r--r--   1 cloudera cloudera         32 2022-01-11 03:13 /user/cloudera/sample.csv
-rw-r--r--   1 cloudera cloudera         31 2022-01-10 01:00 /user/cloudera/sample.txt
drwxr-xr-x   - cloudera cloudera          0 2022-01-16 06:14 /user/cloudera/student
-rw-r--r--   1 cloudera cloudera       1173 2022-01-20 22:44 /user/cloudera/words.txt
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 /user/cloudera/zeyo_dir

cat: `/user/cloudera': Is a directory
[cloudera@quickstart ~]$ ^C
[cloudera@quickstart ~]$ hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.columns="HBASE_ROW_KEY,info:itemno,info:city,info:item,info:date,info:quantity" groceries /user/cloudera/groceries.csv
OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
22/01/23 09:52:55 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x544fa968 connecting to ZooKeeper ensemble=quickstart.cloudera:2181
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.13.0--1, built on 10/04/2017 18:04 GMT
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:host.name=quickstart.cloudera
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_252
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el6_10.x86_64/jre
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/usr/lib/hbase/bin/../conf:/usr/lib/jvm/java-1.8.0-openjdk.x86_64/lib/tools.jar:/usr/lib/hbase/bin/..:/usr/lib/hbase/bin/../lib/activation-1.1.jar:/usr/lib/hbase/bin/../lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hbase/bin/../lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hbase/bin/../lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hbase/bin/../lib/api-util-1.0.0-M20.jar:/usr/lib/hbase/bin/../lib/asm-3.2.jar:/usr/lib/hbase/bin/../lib/avro.jar:/usr/lib/hbase/bin/../lib/aws-java-sdk-bundle-1.11.134.jar:/usr/lib/hbase/bin/../lib/commons-beanutils-1.9.2.jar:/usr/lib/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hbase/bin/../lib/commons-cli-1.2.jar:/usr/lib/hbase/bin/../lib/commons-codec-1.9.jar:/usr/lib/hbase/bin/../lib/commons-collections-3.2.2.jar:/usr/lib/hbase/bin/../lib/commons-compress-1.4.1.jar:/usr/lib/hbase/bin/../lib/commons-configuration-1.6.jar:/usr/lib/hbase/bin/../lib/commons-daemon-1.0.13.jar:/usr/lib/hbase/bin/../lib/commons-digester-1.8.jar:/usr/lib/hbase/bin/../lib/commons-el-1.0.jar:/usr/lib/hbase/bin/../lib/commons-httpclient-3.1.jar:/usr/lib/hbase/bin/../lib/commons-io-2.4.jar:/usr/lib/hbase/bin/../lib/commons-lang-2.6.jar:/usr/lib/hbase/bin/../lib/commons-logging-1.2.jar:/usr/lib/hbase/bin/../lib/commons-math-2.1.jar:/usr/lib/hbase/bin/../lib/commons-math3-3.1.1.jar:/usr/lib/hbase/bin/../lib/commons-net-3.1.jar:/usr/lib/hbase/bin/../lib/core-3.1.1.jar:/usr/lib/hbase/bin/../lib/curator-client-2.7.1.jar:/usr/lib/hbase/bin/../lib/curator-framework-2.7.1.jar:/usr/lib/hbase/bin/../lib/curator-recipes-2.7.1.jar:/usr/lib/hbase/bin/../lib/disruptor-3.3.0.jar:/usr/lib/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/usr/lib/hbase/bin/../lib/gson-2.2.4.jar:/usr/lib/hbase/bin/../lib/guava-12.0.1.jar:/usr/lib/hbase/bin/../lib/hamcrest-core-1.3.jar:/usr/lib/hbase/bin/../lib/hbase-annotations-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-annotations-1.2.0-cdh5.13.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-client-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-common-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-common-1.2.0-cdh5.13.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-examples-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-external-blockcache-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop2-compat-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop2-compat-1.2.0-cdh5.13.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop-compat-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-hadoop-compat-1.2.0-cdh5.13.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-it-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-it-1.2.0-cdh5.13.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-prefix-tree-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-procedure-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-protocol-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-resource-bundle-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-rest-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-rsgroup-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-rsgroup-1.2.0-cdh5.13.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-server-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-server-1.2.0-cdh5.13.0-tests.jar:/usr/lib/hbase/bin/../lib/hbase-shell-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-spark-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/hbase-thrift-1.2.0-cdh5.13.0.jar:/usr/lib/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/usr/lib/hbase/bin/../lib/hsqldb-1.8.0.10.jar:/usr/lib/hbase/bin/../lib/htrace-core-3.2.0-incubating.jar:/usr/lib/hbase/bin/../lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hbase/bin/../lib/htrace-core.jar:/usr/lib/hbase/bin/../lib/httpclient-4.2.5.jar:/usr/lib/hbase/bin/../lib/httpcore-4.2.5.jar:/usr/lib/hbase/bin/../lib/jackson-annotations-2.2.3.jar:/usr/lib/hbase/bin/../lib/jackson-core-2.2.3.jar:/usr/lib/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/usr/lib/hbase/bin/../lib/jackson-databind-2.2.3.jar:/usr/lib/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hbase/bin/../lib/jackson-xc-1.8.8.jar:/usr/lib/hbase/bin/../lib/jamon-runtime-2.4.1.jar:/usr/lib/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/usr/lib/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/usr/lib/hbase/bin/../lib/java-xmlbuilder-0.4.jar:/usr/lib/hbase/bin/../lib/jaxb-api-2.1.jar:/usr/lib/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hbase/bin/../lib/jcodings-1.0.8.jar:/usr/lib/hbase/bin/../lib/jersey-client-1.9.jar:/usr/lib/hbase/bin/../lib/jersey-core-1.9.jar:/usr/lib/hbase/bin/../lib/jersey-json-1.9.jar:/usr/lib/hbase/bin/../lib/jersey-server-1.9.jar:/usr/lib/hbase/bin/../lib/jets3t-0.9.0.jar:/usr/lib/hbase/bin/../lib/jettison-1.3.3.jar:/usr/lib/hbase/bin/../lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hbase/bin/../lib/jetty-sslengine-6.1.26.cloudera.4.jar:/usr/lib/hbase/bin/../lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hbase/bin/../lib/joni-2.1.2.jar:/usr/lib/hbase/bin/../lib/jruby-cloudera-1.0.0.jar:/usr/lib/hbase/bin/../lib/jsch-0.1.42.jar:/usr/lib/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/usr/lib/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/usr/lib/hbase/bin/../lib/jsp-api-2.1.jar:/usr/lib/hbase/bin/../lib/junit-4.12.jar:/usr/lib/hbase/bin/../lib/leveldbjni-all-1.8.jar:/usr/lib/hbase/bin/../lib/libthrift-0.9.3.jar:/usr/lib/hbase/bin/../lib/log4j-1.2.17.jar:/usr/lib/hbase/bin/../lib/metrics-core-2.2.0.jar:/usr/lib/hbase/bin/../lib/netty-all-4.0.23.Final.jar:/usr/lib/hbase/bin/../lib/paranamer-2.3.jar:/usr/lib/hbase/bin/../lib/protobuf-java-2.5.0.jar:/usr/lib/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/usr/lib/hbase/bin/../lib/servlet-api-2.5.jar:/usr/lib/hbase/bin/../lib/slf4j-api-1.7.5.jar:/usr/lib/hbase/bin/../lib/slf4j-log4j12.jar:/usr/lib/hbase/bin/../lib/snappy-java-1.0.4.1.jar:/usr/lib/hbase/bin/../lib/spymemcached-2.11.6.jar:/usr/lib/hbase/bin/../lib/xmlenc-0.52.jar:/usr/lib/hbase/bin/../lib/xz-1.0.jar:/usr/lib/hbase/bin/../lib/zookeeper.jar:/etc/hadoop/conf:/usr/lib/hadoop/lib/aws-java-sdk-bundle-1.11.134.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/httpclient-4.2.5.jar:/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/jsch-0.1.42.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/httpcore-4.2.5.jar:/usr/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/avro.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.2.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/slf4j-log4j12.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.5.jar:/usr/lib/hadoop/lib/hue-plugins-3.9.0-cdh5.13.0.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/zookeeper.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/logredactor-1.0.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop/.//parquet-test-hadoop2.jar:/usr/lib/hadoop/.//parquet-generator.jar:/usr/lib/hadoop/.//parquet-pig.jar:/usr/lib/hadoop/.//parquet-jackson.jar:/usr/lib/hadoop/.//parquet-tools.jar:/usr/lib/hadoop/.//parquet-protobuf.jar:/usr/lib/hadoop/.//parquet-format-javadoc.jar:/usr/lib/hadoop/.//hadoop-aws-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.13.0-tests.jar:/usr/lib/hadoop/.//parquet-hadoop-bundle.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//parquet-format-sources.jar:/usr/lib/hadoop/.//parquet-thrift.jar:/usr/lib/hadoop/.//parquet-scala_2.10.jar:/usr/lib/hadoop/.//parquet-format.jar:/usr/lib/hadoop/.//parquet-cascading.jar:/usr/lib/hadoop/.//hadoop-common-tests.jar:/usr/lib/hadoop/.//parquet-scrooge_2.10.jar:/usr/lib/hadoop/.//parquet-column.jar:/usr/lib/hadoop/.//parquet-hadoop.jar:/usr/lib/hadoop/.//parquet-common.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-nfs-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop/.//hadoop-auth-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop/.//parquet-pig-bundle.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-common-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop/.//parquet-avro.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop/.//parquet-encoding.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-el-1.0.jar:/usr/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/jsp-api-2.1.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.6.0-cdh5.13.0-tests.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/spark-1.6.0-cdh5.13.0-yarn-shuffle.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/jline-2.11.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/spark-yarn-shuffle.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.8.8.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/avro.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.4.1.jar:/usr/lib/hadoop-mapreduce/.//okio-1.4.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.42.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.2.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//junit-4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//avro.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//okhttp-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-el-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//jasper-runtime-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//mockito-all-1.8.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.6.0-cdh5.13.0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jasper-compiler-5.5.23.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//zookeeper.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.6.0-cdh5.13.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.8.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/etc/hadoop/conf/usr/lib/hadoop/*:/usr/lib/hadoop/lib/aws-java-sdk-bundle-1.11.134.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/httpclient-4.2.5.jar:/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-httpclient-3.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/jsch-0.1.42.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/httpcore-4.2.5.jar:/usr/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/avro.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.2.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/slf4j-log4j12.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.5.jar:/usr/lib/hadoop/lib/hue-plugins-3.9.0-cdh5.13.0.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/zookeeper.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/logredactor-1.0.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.13.0.jar:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/lib/netty-3.10.5.Final.jar:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/lib/jline-2.11.jar:/usr/lib/zookeeper/lib/log4j-1.2.16.jar:/usr/lib/zookeeper/lib/slf4j-log4j12.jar:/usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar:
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/usr/lib/hadoop/lib/native:/usr/lib/hbase/bin/../lib/native/Linux-amd64-64
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:os.version=2.6.32-573.el6.x86_64
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:user.name=cloudera
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:user.home=/home/cloudera
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Client environment:user.dir=/home/cloudera
22/01/23 09:52:55 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=quickstart.cloudera:2181 sessionTimeout=60000 watcher=hconnection-0x544fa9680x0, quorum=quickstart.cloudera:2181, baseZNode=/hbase
22/01/23 09:52:55 INFO zookeeper.ClientCnxn: Opening socket connection to server quickstart.cloudera/192.168.230.128:2181. Will not attempt to authenticate using SASL (unknown error)
22/01/23 09:52:55 INFO zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.230.128:60136, server: quickstart.cloudera/192.168.230.128:2181
22/01/23 09:52:55 INFO zookeeper.ClientCnxn: Session establishment complete on server quickstart.cloudera/192.168.230.128:2181, sessionid = 0x17e870363f904a7, negotiated timeout = 60000
22/01/23 09:53:00 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
22/01/23 09:53:01 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x17e870363f904a7
22/01/23 09:53:01 INFO zookeeper.ClientCnxn: EventThread shut down
22/01/23 09:53:01 INFO zookeeper.ZooKeeper: Session: 0x17e870363f904a7 closed
22/01/23 09:53:01 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/192.168.230.128:8032
22/01/23 09:53:02 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
22/01/23 09:53:08 INFO input.FileInputFormat: Total input paths to process : 1
22/01/23 09:53:09 INFO mapreduce.JobSubmitter: number of splits:1
22/01/23 09:53:09 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
22/01/23 09:53:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1642942813896_0001
22/01/23 09:53:13 INFO impl.YarnClientImpl: Submitted application application_1642942813896_0001
22/01/23 09:53:14 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1642942813896_0001/
22/01/23 09:53:14 INFO mapreduce.Job: Running job: job_1642942813896_0001
22/01/23 09:53:58 INFO mapreduce.Job: Job job_1642942813896_0001 running in uber mode : false
22/01/23 09:53:58 INFO mapreduce.Job:  map 0% reduce 0%
22/01/23 09:54:36 INFO mapreduce.Job:  map 100% reduce 0%
22/01/23 09:54:37 INFO mapreduce.Job: Job job_1642942813896_0001 completed successfully
22/01/23 09:54:38 INFO mapreduce.Job: Counters: 31
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=185128
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=552
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=2
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=16635904
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=32492
		Total vcore-milliseconds taken by all map tasks=32492
		Total megabyte-milliseconds taken by all map tasks=16635904
	Map-Reduce Framework
		Map input records=14
		Map output records=0
		Input split bytes=124
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=539
		CPU time spent (ms)=1960
		Physical memory (bytes) snapshot=104054784
		Virtual memory (bytes) snapshot=1944952832
		Total committed heap usage (bytes)=50724864
	ImportTsv
		Bad Lines=14
	File Input Format Counters 
		Bytes Read=428
	File Output Format Counters 
		Bytes Written=0
[cloudera@quickstart ~]$ 
hbase(main):005:0> scan 'groceries'
ROW                                             COLUMN+CELL                                                                                                                                
 o1                                             column=info:city, timestamp=1642764326730, value=Bananas                                                                                   
 o1                                             column=info:date, timestamp=1642764326730, value=7                                                                                         
 o1                                             column=info:item, timestamp=1642764326730, value=01-01-2017                                                                                
 o1                                             column=info:itemno, timestamp=1642764326730, value=Seattle                                                                                 
 o10                                            column=info:city, timestamp=1642764326730, value=Onion                                                                                     
 o10                                            column=info:date, timestamp=1642764326730, value=4                                                                                         
 o10                                            column=info:item, timestamp=1642764326730, value=06-01-2017                                                                                
 o10                                            column=info:itemno, timestamp=1642764326730, value=Issaquah                                                                                
 o11                                            column=info:city, timestamp=1642764326730, value=Bread                                                                                     
 o11                                            column=info:date, timestamp=1642764326730, value=5                                                                                         
 o11                                            column=info:item, timestamp=1642764326730, value=05-01-2017                                                                                
 o11                                            column=info:itemno, timestamp=1642764326730, value=Renton                                                                                  
 o12                                            column=info:city, timestamp=1642764326730, value=Onion                                                                                     
 o12                                            column=info:date, timestamp=1642764326730, value=4                                                                                         
 o12                                            column=info:item, timestamp=1642764326730, value=07-01-2017                                                                                
 o12                                            column=info:itemno, timestamp=1642764326730, value=Issaquah                                                                                
 o13                                            column=info:city, timestamp=1642764326730, value=Bread                                                                                     
 o13                                            column=info:date, timestamp=1642764326730, value=5                                                                                         
 o13                                            column=info:item, timestamp=1642764326730, value=07-01-2017                                                                                
 o13                                            column=info:itemno, timestamp=1642764326730, value=Sammamish                                                                               
 o14                                            column=info:city, timestamp=1642764326730, value=Tomato                                                                                    
 o14                                            column=info:date, timestamp=1642764326730, value=6                                                                                         
 o14                                            column=info:item, timestamp=1642764326730, value=07-01-2017                                                                                
 o14                                            column=info:itemno, timestamp=1642764326730, value=Issaquah                                                                                
 o2                                             column=info:city, timestamp=1642764326730, value=Apples                                                                                    
 o2                                             column=info:date, timestamp=1642764326730, value=20                                                                                        
 o2                                             column=info:item, timestamp=1642764326730, value=02-01-2017                                                                                
 o2                                             column=info:itemno, timestamp=1642764326730, value=Kent                                                                                    
 o3                                             column=info:city, timestamp=1642764326730, value=Flowers                                                                                   
 o3                                             column=info:date, timestamp=1642764326730, value=10                                                                                        
 o3                                             column=info:item, timestamp=1642764326730, value=02-01-2017                                                                                
 o3                                             column=info:itemno, timestamp=1642764326730, value=Bellevue                                                                                
 o4                                             column=info:city, timestamp=1642764326730, value=Meat                                                                                      
 o4                                             column=info:date, timestamp=1642764326730, value=40                                                                                        
 o4                                             column=info:item, timestamp=1642764326730, value=03-01-2017                                                                                
 o4                                             column=info:itemno, timestamp=1642764326730, value=Redmond                                                                                 
 o5                                             column=info:city, timestamp=1642764326730, value=Potatoes                                                                                  
 o5                                             column=info:date, timestamp=1642764326730, value=9                                                                                         
 o5                                             column=info:item, timestamp=1642764326730, value=04-01-2017                                                                                
 o5                                             column=info:itemno, timestamp=1642764326730, value=Seattle                                                                                 
 o6                                             column=info:city, timestamp=1642764326730, value=Bread                                                                                     
 o6                                             column=info:date, timestamp=1642764326730, value=5                                                                                         
 o6                                             column=info:item, timestamp=1642764326730, value=04-01-2017                                                                                
 o6                                             column=info:itemno, timestamp=1642764326730, value=Bellevue                                                                                
 o7                                             column=info:city, timestamp=1642764326730, value=Bread                                                                                     
 o7                                             column=info:date, timestamp=1642764326730, value=5                                                                                         
 o7                                             column=info:item, timestamp=1642764326730, value=05-01-2017                                                                                
 o7                                             column=info:itemno, timestamp=1642764326730, value=Redmond                                                                                 
 o8                                             column=info:city, timestamp=1642764326730, value=Onion                                                                                     
 o8                                             column=info:date, timestamp=1642764326730, value=4                                                                                         
 o8                                             column=info:item, timestamp=1642764326730, value=05-01-2017                                                                                
 o8                                             column=info:itemno, timestamp=1642764326730, value=Issaquah                                                                                
 o9                                             column=info:city, timestamp=1642764326730, value=Cheese                                                                                    
 o9                                             column=info:date, timestamp=1642764326730, value=15                                                                                        
 o9                                             column=info:item, timestamp=1642764326730, value=05-01-2017                                                                                
 o9                                             column=info:itemno, timestamp=1642764326730, value=Redmond                                                                                 
14 row(s) in 1.4130 seconds

